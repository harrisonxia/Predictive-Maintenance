{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iot_project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrisonxia/Predictive-Maintenance/blob/master/iot_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Otca1DWC36ip",
        "colab_type": "code",
        "outputId": "b2c19cf5-2909-4119-8015-81d4d2a3b20c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e913b332-f655-44c2-9eea-037e3938c1e8\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e913b332-f655-44c2-9eea-037e3938c1e8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving part-00000-9bbc2c66-17ba-4260-964c-d4f40fd03234-c000.csv to part-00000-9bbc2c66-17ba-4260-964c-d4f40fd03234-c000.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E_y2moxK6VJO",
        "colab_type": "code",
        "outputId": "56ae287b-d536-41fe-fe1e-8414ac40ce49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import placeholder, float32, reshape, Variable, random_normal, matmul, global_variables_initializer\n",
        "from tensorflow import device, Session, reset_default_graph, reduce_sum, square\n",
        "from tensorflow.train import AdamOptimizer\n",
        "from tensorflow.nn import tanh, dynamic_rnn\n",
        "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "def loadCSV():\n",
        "  df = pd.read_csv(io.BytesIO(uploaded['part-00000-9bbc2c66-17ba-4260-964c-d4f40fd03234-c000.csv'])).sort_values(\n",
        "\t ['TimeStamp'], ascending = True).reset_index()\n",
        "\n",
        "  df['TimeStamp'] = pd.to_datetime(df['TimeStamp'])\n",
        "  \n",
        "  df.drop(['::[scararobot]Ax_J1.PositionCommand','::[scararobot]Ax_J1.TorqueFeedback','::[scararobot]Ax_J2.PositionCommand','::[scararobot]Ax_J2.TorqueFeedback','::[scararobot]Ax_J3.TorqueFeedback','::[scararobot]Ax_J6.TorqueFeedback','::[scararobot]ScanTimeAverage','::[scararobot]Ax_J6.PositionCommand','::[scararobot]Ax_J3.PositionCommand','index'], axis=1, inplace=True)\n",
        "  df['Total']= df.select_dtypes(include=['float64','float32']).apply(lambda row: np.sum(row),axis=1)\n",
        "  return pd.Series(df['Total'])\n",
        "\n",
        "def featureSelect(df):\n",
        "  x = df.iloc[0:, 1: 20].values\n",
        "  y = df.iloc[0:, 20: 21].values.flatten()\n",
        "  y = np.array(y).astype(int)\n",
        "  \n",
        "  SelBest = SelectKBest(chi2, k = 10)\n",
        "  SelBest.fit_transform(x, y)\n",
        "  choosed_features = list(SelBest.get_support())\n",
        "  f_df = df.iloc[0:, 0: 1]\n",
        "  for index, fea in enumerate(choosed_features):\n",
        "    if fea == True:\n",
        "      f_df[str(index)] = df.iloc[0:, index]\n",
        "  \n",
        "  f_df['Total'] = f_df.select_dtypes(include = ['float64', 'float32']).apply(lambda row: np.sum(row), axis = 1)\n",
        "  tensor = pd.Series(f_df['Total'])\n",
        "  return tensor\n",
        "\n",
        "def prepareData(num_periods, f_horizon, TS, test_size = 1):\n",
        "  #create our training input data set \"X\"\n",
        "  x_data = TS[:(len(TS)-(len(TS) % num_periods))]\n",
        "  x_batches = x_data.reshape(-1, num_periods, 1)\n",
        "\n",
        "  #create our training output dataset \"y\"\n",
        "  y_data = TS[1:(len(TS)-(len(TS) % num_periods))+f_horizon]\n",
        "  print (y_data)\n",
        "  print (len(y_data))\n",
        "  y_batches = y_data.reshape(-1, num_periods, 1)\n",
        "  print (len(y_batches))\n",
        "  \n",
        "  #create our test X and y data\n",
        "  test_x_setup = TS[-(num_periods * test_size + f_horizon):]\n",
        "  testX = test_x_setup[:num_periods * test_size].reshape(-1, num_periods, 1)\n",
        "  testY = TS[-(num_periods * test_size):].reshape(-1, num_periods, 1)\n",
        "  print (testX.shape)\n",
        "  print (testX[:,(num_periods-1):num_periods])\n",
        "  print (testY.shape)\n",
        "  print (testY[:,(num_periods-1):num_periods])\n",
        "  return x_batches, y_batches, testX, testY\n",
        "\n",
        "def RNN_LSTM(num_periods, input_size, hidden_number, output_size):\n",
        "  \n",
        "  X = placeholder(float32, [None, num_periods, input_size], name = \"X\")   #create variable objects\n",
        "  Y = placeholder(float32, [None, num_periods, output_size], name = \"Y\")\n",
        "\n",
        "  LSTM_cell_1 = LSTMCell(num_units = hidden_number, activation = tanh)\n",
        "  LSTM_cell_2 = LSTMCell(num_units = 100)\n",
        "  stacked_lstm_cell = MultiRNNCell([LSTM_cell_1, LSTM_cell_2])\n",
        "  lstm_output, states = dynamic_rnn(stacked_lstm_cell, X, dtype=float32)               #choose dynamic over static\n",
        "  \n",
        "  stacked_rnn_output = reshape(lstm_output, [-1, hidden_number])           #change the form into a tensor\n",
        "  weight = Variable(random_normal([100, 1]))   \n",
        "  bias = Variable(random_normal([1]))   \n",
        "  stacked_outputs = matmul(stacked_rnn_output, weight) + bias  \n",
        "  f_outputs = reshape(stacked_outputs, [-1, num_periods, output_size])          #shape of results\n",
        "  return f_outputs, X, Y\n",
        "\n",
        "def Train(lr, train_op, cost, epochs, dev, X, Y, x_batches, y_batches, X_test, f_outputs):\n",
        "  init_g = global_variables_initializer()\n",
        "  loss = []\n",
        "  with device(dev):\n",
        "    with Session() as session:\n",
        "      session.run(init_g)\n",
        "      for ep in range(epochs):\n",
        "        session.run(train_op, feed_dict = {X: x_batches, Y: y_batches})\n",
        "        if ep % 10 == 0:\n",
        "          mse = cost.eval(session = session, feed_dict = {X: x_batches, Y: y_batches})\n",
        "          mse = math.sqrt(mse)\n",
        "          if ep % 100 == 0:\n",
        "            print(ep, \"\\tMSE:\", mse)\n",
        "          loss.append(mse)\n",
        "      y_pred = session.run(f_outputs, feed_dict = {X: X_test})\n",
        "  return session, y_pred, loss\n",
        "\n",
        "#Plot our test y data and our y-predicted forecast\n",
        "def plotPred(y_pred, Y_test):\n",
        "  plt.title(\"Forecast vs Actual\", fontsize=14)\n",
        "  plt.plot(pd.Series(np.ravel(Y_test)), \"bo\", markersize=10, label=\"Actual\")\n",
        "  plt.plot(pd.Series(np.ravel(y_pred)), \"r.\", markersize=10, label=\"Forecast\")\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.xlabel(\"Time Periods\")\n",
        "  plt.show()\n",
        "  return\n",
        "\n",
        "def plotLoss(loss):\n",
        "  plt.title(\"Training Loss\", fontsize=14)\n",
        "  plt.plot(loss)\n",
        "  plt.xlabel(\"Time Periods\")\n",
        "  plt.show()\n",
        "  return\n",
        "\n",
        "#anomaly detection\n",
        "def anomalyDetection(trainset, pred, outliers_fraction, model = 'SVM'):\n",
        "  if model == 'SVM':\n",
        "    m = OneClassSVM(nu=0.95 * outliers_fraction, kernel=\"rbf\",gamma='scale')\n",
        "  elif model == 'IF':\n",
        "    m = IsolationForest(contamination = outliers_fraction)\n",
        "  m.fit(trainset)\n",
        "  column = 'anomaly_' + model\n",
        "  pred[column] = pd.Series(m.predict(pred))\n",
        "  result = pred.loc[pred[column] == -1, ['Prediction']]\n",
        "  pred = pred.drop(columns = [column])\n",
        "  return result, pred\n",
        "\n",
        "def plotAD(pred, SVM, IF):\n",
        "  fig, ax = plt.subplots(figsize=(15,9))\n",
        "\n",
        "  ax.plot(pred['Prediction'], color='blue', label = 'Prediction')\n",
        "  ax.scatter(list(SVM.index.values), list(SVM.Prediction.values), color = 'red', label = 'anomalySVM')\n",
        "  ax.scatter(list(IF.index.values), list(IF.Prediction.values), color = 'green', label = 'anomalyIF')\n",
        "  plt.legend(loc = 'upper right')\n",
        "  plt.show()\n",
        "  return\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  #tensor = featureSelect(loadCSV())\n",
        "  tensor = loadCSV()\n",
        "  num_periods = 100\n",
        "  f_horizon = 1       #number of periods into the future we are forecasting\n",
        "  TS = np.array(tensor)   #convert time series object to an array\n",
        "  \n",
        "  x_batches, y_batches, X_test, Y_test = prepareData(num_periods, f_horizon, TS)\n",
        "  \n",
        "  #set up model parameters\n",
        "  reset_default_graph()   #reset graph\n",
        "  input_size = 1            #number of vectors submitted\n",
        "  hidden_number = 100          #number of neurons\n",
        "  output_size = 1            #number of output vectors\n",
        "\n",
        "  f_outputs, X, Y = RNN_LSTM(num_periods, input_size, hidden_number, output_size)\n",
        "  \n",
        "  #set up training parameters\n",
        "  learning_rate = 0.005   #learning rate\n",
        "  cost = reduce_sum(square(f_outputs - Y), name = 'cost')    #define the cost function\n",
        "  optimizer = AdamOptimizer(learning_rate = learning_rate)          #gradient descent method\n",
        "  train_op = optimizer.minimize(cost)          #train the result of the application of the cost_function                                 \n",
        "  epochs = 1000     #number of iterations or training cycles, includes both the FeedFoward and Backpropogation\n",
        "  \n",
        "  session, y_pred, loss = Train(learning_rate, train_op, cost, epochs, '/device:GPU:0', X, Y, x_batches, y_batches, X_test, f_outputs)\n",
        "\n",
        "  DIR=\"/usr/tmp/IoTModel\"  #path where the model will be saved\n",
        "  #tf.saved_model.simple_save(session, DIR,\n",
        "  #                         inputs={\"X\": X},\n",
        "  #                         outputs={\"outputs\": f_outputs})\n",
        "  \n",
        "  plotPred(y_pred, Y_test)\n",
        "  plotLoss(loss)\n",
        "  \n",
        "  #prepare for anomaly detection\n",
        "  pred = pd.DataFrame(np.ravel(y_pred), columns = ['Prediction'])\n",
        "  scaler = StandardScaler()\n",
        "  np_scaled = scaler.fit_transform(pred)\n",
        "  pred = pd.DataFrame(np_scaled, columns = ['Prediction'])\n",
        "  \n",
        "  testDF = pd.DataFrame(TS, columns = ['Total'])\n",
        "  testDF_scaled = scaler.fit_transform(testDF)\n",
        "  testDF = pd.DataFrame(testDF_scaled, columns = ['Total'])\n",
        "  \n",
        "  SVM, pred = anomalyDetection(testDF, pred, 0.002)\n",
        "  IF, pred = anomalyDetection(testDF, pred, 0.01, 'IF')\n",
        "  plotAD(pred, SVM, IF)\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 327.414406    0.        327.791445 ...    0.       -128.657941\n",
            "    0.      ]\n",
            "81900\n",
            "819\n",
            "(1, 100, 1)\n",
            "[[[0.]]]\n",
            "(1, 100, 1)\n",
            "[[[4.029848]]]\n",
            "0 \tMSE: 42929.79496806385\n",
            "100 \tMSE: 25576.38410721891\n",
            "200 \tMSE: 19626.49556084835\n",
            "300 \tMSE: 15275.65697441521\n",
            "400 \tMSE: 12285.643654282017\n",
            "500 \tMSE: 10144.410874959669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DdeVGMy0xn2U",
        "colab_type": "code",
        "outputId": "8ddeb2b5-25dc-4b4e-aa3f-ba0232750100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        }
      },
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Prediction</th>\n",
              "      <th>anomaly_SVM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.610533</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.439323</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.351026</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.369972</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.351380</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.655158</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.342733</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.455347</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.339639</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.519618</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.345878</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.486042</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.352215</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.343447</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.352243</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.238792</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.351173</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.138608</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.349572</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.994279</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.344579</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.888944</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.342521</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.736574</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.348380</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.302520</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.339153</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>-0.041034</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.338810</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>-0.326210</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>0.317919</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>-1.070870</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.350427</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>-0.861820</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>0.346893</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>-0.843280</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>0.346110</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>-0.801681</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>0.345621</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>-0.733656</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>0.344409</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>-0.566408</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>0.344019</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>-0.427429</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.343658</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>-0.265438</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0.345243</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>-0.146339</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.345070</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>-0.006880</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>0.345717</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0.080095</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.343913</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>0.181641</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>0.345826</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.258704</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.345141</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.326282</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.342341</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.415526</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Prediction  anomaly_SVM\n",
              "0     0.610533            1\n",
              "1     3.439323           -1\n",
              "2     0.351026            1\n",
              "3     2.369972           -1\n",
              "4     0.351380            1\n",
              "5     1.655158            1\n",
              "6     0.342733            1\n",
              "7     1.455347            1\n",
              "8     0.339639            1\n",
              "9     1.519618            1\n",
              "10    0.345878            1\n",
              "11    1.486042            1\n",
              "12    0.352215            1\n",
              "13    1.343447            1\n",
              "14    0.352243            1\n",
              "15    1.238792            1\n",
              "16    0.351173            1\n",
              "17    1.138608            1\n",
              "18    0.349572            1\n",
              "19    0.994279            1\n",
              "20    0.344579            1\n",
              "21    0.888944            1\n",
              "22    0.342521            1\n",
              "23    0.736574            1\n",
              "24    0.348380            1\n",
              "25    0.302520            1\n",
              "26    0.339153            1\n",
              "27   -0.041034            1\n",
              "28    0.338810            1\n",
              "29   -0.326210            1\n",
              "..         ...          ...\n",
              "70    0.317919            1\n",
              "71   -1.070870           -1\n",
              "72    0.350427            1\n",
              "73   -0.861820            1\n",
              "74    0.346893            1\n",
              "75   -0.843280            1\n",
              "76    0.346110            1\n",
              "77   -0.801681            1\n",
              "78    0.345621            1\n",
              "79   -0.733656            1\n",
              "80    0.344409            1\n",
              "81   -0.566408            1\n",
              "82    0.344019            1\n",
              "83   -0.427429            1\n",
              "84    0.343658            1\n",
              "85   -0.265438            1\n",
              "86    0.345243            1\n",
              "87   -0.146339            1\n",
              "88    0.345070            1\n",
              "89   -0.006880            1\n",
              "90    0.345717            1\n",
              "91    0.080095            1\n",
              "92    0.343913            1\n",
              "93    0.181641            1\n",
              "94    0.345826            1\n",
              "95    0.258704            1\n",
              "96    0.345141            1\n",
              "97    0.326282            1\n",
              "98    0.342341            1\n",
              "99    0.415526            1\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}